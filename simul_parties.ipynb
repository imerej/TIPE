{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7e08f81aab90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#permet davoir des resultats reproducibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    #action entier entre 0 et 8\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    #state.reshape(2,3) transforme un tenseur en matrice 2 par 3\n",
    "    #state.reshape(-1) laisse pytorch decider quelle est la bonne taille\n",
    "    #renvoie un tableau de booleens selon si on peut jouer dans une case ou non\n",
    "    # mais cest converti en 0 ou 1 par astype \n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            #state[row, :] renvoie un tab contenant toute la ligne row\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state\n",
    "    #ceci transforme le plateau (tab) en trois tab : j1, j2, vide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01448125 0.49115843 0.67838809]\n",
      " [0.67212423 0.0539281  0.25638872]\n",
      " [0.62807648 0.49253838 0.08341415]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab = np.random.rand(3,3)\n",
    "print (tab)\n",
    "tab[(2,0)]\n",
    "any([-1,-1])\n",
    "() + (0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le but netant pas de jouer mais detablier lexistence de strategie\n",
    "# gagnante, on peut implementer une version peu intuitive mais plus simple a implementer et donc en 2D\n",
    "import itertools\n",
    "\n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "class NDGame:\n",
    "    def __init__(self, length, dim):\n",
    "        self.length = length # elgnth as in the length of the side of the hypercube\n",
    "        self.dim = dim\n",
    "        self.action_size = self.length ** self.dim\n",
    "\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        dimensions = (self.length, ) * self.dim #dim-uplet fait de chaque bord de lhypercube\n",
    "        return np.zeros(dimensions) \n",
    "            #jimplemente un matrice taille^n mais je la transformerai en 2D plus tard\n",
    "            \n",
    "            #implementation par hypergraphe etait lautre moyen \n",
    "            #on aurait fait ca pour garder la pertinence de conv2D\n",
    "            #la on met la fonction arete de william pour inserer des 2\n",
    "            \n",
    "    def to_base(self, n, base):\n",
    "        decomp = []\n",
    "        for _ in range(self.dim):\n",
    "            decomp.append(n % base)\n",
    "            n //= base\n",
    "        return tuple(reversed(decomp))\n",
    "\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        indices = self.to_base(action, self.length)\n",
    "        state[indices] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "        #transforme mon tableau n dimensionsel en tab 1 D trouve les cases libres (==0) \n",
    "        #ca renvoie un tableau de 1 et 0 selon si la case est libre\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "\n",
    "    def get_directions(self):\n",
    "        return [d for d in itertools.product([-1, 0, 1], repeat=self.dim) if any(d)]\n",
    "        #any regarde si il ny a que des zero (cest un OU pour chaque element du tableau en quelque sorte)\n",
    "    \n",
    "    def in_bounds(self, indices):\n",
    "        return all(0 <= i < self.length for i in indices)\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        if action is None:\n",
    "            return False\n",
    "        \n",
    "        indices_action = self.to_base(action, self.length)\n",
    "        player = state[indices_action]\n",
    "        for direction in self.get_directions():\n",
    "            indices_avant = indices_action\n",
    "            indices_arriere = indices_action\n",
    "            alignes = 1\n",
    "            for i in range(self.length):\n",
    "                \n",
    "                indices_avant = tuple((x + y)  for x, y in zip(indices_avant, direction))\n",
    "                if self.in_bounds(indices_avant):\n",
    "                    alignes += (state[indices_avant] == player)\n",
    "                    \n",
    "                indices_arriere = tuple((x - y)  for x, y in zip(indices_arriere, direction))\n",
    "                if self.in_bounds(indices_arriere):\n",
    "                    alignes += (state[indices_arriere] == player)\n",
    "                    \n",
    "            if alignes >= self.length :\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        state = state.reshape(self.length,-1)\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "\n",
      "\n",
      "[[[ 1.  0.  1.]\n",
      "  [ 1. -1. -1.]\n",
      "  [ 1.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]]]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nd = NDGame(3,3)\n",
    "etat = nd.get_initial_state()\n",
    "print(etat)\n",
    "etat = nd.get_next_state(etat, 0, 1)\n",
    "etat = nd.get_next_state(etat, 2, 1)\n",
    "etat = nd.get_next_state(etat, 3, 1)\n",
    "etat = nd.get_next_state(etat, 7, 1)\n",
    "etat = nd.get_next_state(etat, 4, -1)\n",
    "etat = nd.get_next_state(etat, 5, -1)\n",
    "etat = nd.get_next_state(etat, 6, 1)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(etat)\n",
    "\n",
    "arr = nd.get_valid_moves(etat)\n",
    "indices = np.where(arr == 1)[0]        # prend les indices valant 1\n",
    "random_index = np.random.choice(indices)\n",
    "print(random_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        #nb de convolutional block ie longueur du backbone\n",
    "        # et leur taille\n",
    "        \n",
    "        super().__init__()\n",
    "        #super verifie que tout le system est correctement mis en place\n",
    "        #ie il inititalise nn.module\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            #cest une couche convolutionnelle\n",
    "            \n",
    "            #input channels\n",
    "            #le premier 3 permet de prendre trois grilles en parametre : j1, j2, vide\n",
    "            \n",
    "            #output channels\n",
    "            #num hidden est le nb de filtres, de feature maps\n",
    "            \n",
    "            # idee : implementer en n dimension ?\n",
    "            \n",
    "            #kernel size donne la taille des filtres ie la taille de lendroit zoomé\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            #normalisation augmente la vitesee dentrainement askip\n",
    "            nn.ReLU()\n",
    "            #enleve les valeurs negatives\n",
    "        )\n",
    "        #nn.sequential permet de definir une sequence de fonctions a appliquer\n",
    "        #ca cree donc un nouveau bloc comme suite des trois differents blocs\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "            #crée num_resBlocks residual blocks, chacun avec num_hidden filtres,\n",
    "            #et les garde comme liste de modules a entrainer\n",
    "            #comme partie principale du nn : comme backbone\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            # 4D : (batch_size, channels, height, width) -> 1D\n",
    "            nn.Linear(32 * game.action_size, game.action_size)\n",
    "            #fully connected layer\n",
    "        )#32 est arbitraire mais empiriquement pas mauvais\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.action_size, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        #deplace le modele vers le device : GPU/CPU\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "        #on applique le saut : sortie = entree + F(entree) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior = 0, visit_count = 0):\n",
    "        \n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        #prior : proba d'une action du pov du parent\n",
    "        self.children = []\n",
    "        self.visit_count = visit_count\n",
    "        \"\"\"\n",
    "        self.expandable_moves = game.get_valid_moves(state)\n",
    "        #car on expande tout dun coup mtn\n",
    "        \"\"\"\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0 # and np.sum(self.expandable_moves) == 0 \n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value  = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        # 1 - ... permet de renverser les roles parents/enfant adversaires \n",
    "        #return q_value + self.args['C'] * (math.sqrt(math.log(self.visit_count) / child.visit_count)\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "        return child\n",
    "        \"\"\"\n",
    "        action = np.random.choice(np.where(self.expandable_moves == 1)[0])\n",
    "        # [0] tranforme le n - uplet renvoyé en un tab auquel rand choice est appliquable\n",
    "        self.expandable_moves[action] = 0\n",
    "        #indique que la case est desormais plus jouable\n",
    "        \"\"\"\n",
    "        \n",
    "                    \n",
    "        \n",
    "    \"\"\"\n",
    "    def simulate(self):\n",
    "        value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        \n",
    "        if is_terminal:\n",
    "            return value\n",
    "        \n",
    "        rollout_state = self.state.copy()\n",
    "        rollout_player = 1\n",
    "        #rollout state signifie etat de reference, courant, initial \n",
    "        while True:\n",
    "            valid_moves = self.game.get_valid_moves(rollout_state)\n",
    "            action = np.random.choice(np.where(valid_moves == 1)[0])\n",
    "            rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
    "            value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n",
    "            if is_terminal:\n",
    "                if rollout_player == -1:\n",
    "                    value = self.game.get_opponent_value(value)\n",
    "                return value    \n",
    "            \n",
    "            rollout_player = self.game.get_opponent(rollout_player)\n",
    "    \"\"\"\n",
    "    \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad() \n",
    "    # car on ne souhaite pas stocker le parcours effectué sur le \n",
    "    # gradient mais juste lutiliser pour obtenir policy,val\n",
    "    # enleve lautograd \n",
    "    #gain de memoire et de vitesse\n",
    "    \n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count = 1)\n",
    "        #initialise visit count a 1 car on letend apres\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device = self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis = 1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) \\\n",
    "            * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        # P(s,a) = (1-epsilon) (p\\a) + epsilon (eta\\a)\n",
    "        #les lignes precedentes servent a introduire du bruit\n",
    "        #pour avoir + d'aleatoire et + explorer\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves \n",
    "        policy /= np.sum(policy)    \n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            #selection\n",
    "            node = root\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "                \n",
    "            #mesure pour la racine     \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state),device = self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                #pour eviter de jouer la ou on a pas le droit\n",
    "                \n",
    "                policy *= valid_moves #ca met a 0 les coups illegaux\n",
    "                policy /= np.sum(policy) # ca remet une proba\n",
    "                \n",
    "                value = value.item()\n",
    "                 \n",
    "                #expansion\n",
    "                node = node.expand(policy)\n",
    "                \n",
    "                '''\n",
    "                #simulation\n",
    "                value = node.simulate()\n",
    "                '''\n",
    "                \n",
    "            #retropropagation\n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "        #on veut maintenant extraire les resulatats : le vecteur de proba\n",
    "        #on renvoie donc le nb de visite effectuées sur un sommet divisé par le nombre total de visites\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            #car on veut tjrs etre le joueur 1 pour mcts\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            #la temperature permet de modifier durant le processus les coeff dexploration-optimisation\n",
    "            #si la temperature est est superieure a 1, on explore plus : car la puissance va exacerber ou au contraire reduire les differences\n",
    "            #plus la temperature est basse, on optimise\n",
    "            temperature_action_probs = temperature_action_probs / np.sum(temperature_action_probs)\n",
    "            #car il faut renormaliser derriere\n",
    "            \n",
    "            action = np.random.choice(self.game.action_size, p = temperature_action_probs)\n",
    "            #choisi une action au hasard selon la distribution de proba\n",
    "            \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state), #encode letat\n",
    "                        hist_action_probs, #change pas\n",
    "                        hist_outcome #indique le gagnant du pov du joueur courant\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "                \n",
    "            \n",
    "    \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
    "            # sample correspond a tous les elemnts de memory dindices\n",
    "            # entre batchIdx et batchIdx + batch size\n",
    "            \n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            # names = ['Alice', 'Bob', 'Charlie']\n",
    "            # ages = [25, 30, 35]\n",
    "            # zipped = zip(names, ages)\n",
    "            # print(list(zipped))\n",
    "            # [('Alice', 25), ('Bob', 30), ('Charlie', 35)]\n",
    "\n",
    "\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device = self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device = self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device = self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad() # change to self.optimizer\n",
    "            loss.backward()\n",
    "            self.optimizer.step() # change to self.optimizer\n",
    "            \n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            \n",
    "            memory = []\n",
    "            # on stock ici etat, proba, et recompense\n",
    "            \n",
    "            self.model.eval()\n",
    "            # desactive les drop out, ie la mise a zero aleatoires de neurons evitant le surapprentissage\n",
    "            # contraire de model.train()\n",
    "            \n",
    "            # During training, batch norm layers compute the mean \n",
    "            # and variance of the current batch to normalize the input.\n",
    "\n",
    "            # During evaluation, batch norm uses running estimates (computed during \n",
    "            # training) instead of the current batch statistics.\n",
    "            \n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                # on stock les donnees de la partie\n",
    "            # trange donne une visualisation de la progression dans la boucle\n",
    "            \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.args['num_searches']}_{self.args['num_iterations']}_{self.args['num_selfPlay_iterations']}_{self.args['num_epochs']}_{self.args['nom']}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.args['num_searches']}_{self.args['num_iterations']}_{self.args['num_selfPlay_iterations']}_{self.args['num_epochs']}_{self.args['nom']}.pt\")\n",
    "            # saves the states to a disk file\n",
    "            # torch.save(obj, f, pickle_module=pickle, pickle_protocol=2, _use_new_zipfile_serialization=True)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1000/1000 [14:51:10<00:00, 53.47s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:25<00:00,  2.59s/it]\n",
      "100%|███████████████████████████████████████| 1000/1000 [13:55<00:00,  1.20it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:24<00:00,  2.46s/it]\n",
      "100%|███████████████████████████████████████| 1000/1000 [24:43<00:00,  1.48s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [01:08<00:00,  6.81s/it]\n",
      "100%|███████████████████████████████████████| 1000/1000 [14:07<00:00,  1.18it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:20<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "game = NDGame(3,2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#implementation ratée : incompatible avece cuda qui est utilisé par pytorch\n",
    "#GPU Intel intégré (driver i915) Intel UHD Graphics\n",
    "#GPU vs CPU\n",
    "# CPU : 10 coeurs chacun faisants differentes operations en //\n",
    "# GPU : 5000 threads chacune faisant a + b mais avec differents a,b\n",
    "#cuda : plateforme de calcul en parallele de NVIDIA\n",
    " \n",
    "model = ResNet(game, 4, 64, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 200,\n",
    "    'num_iterations': 4,\n",
    "    'num_selfPlay_iterations': 1000,\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3,\n",
    "    'nom' : \"3²\"\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = NDGame(3,2)\n",
    "player = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet(game, 4, 64, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 1000,\n",
    "    'dirichlet_epsilon': 0.,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = game.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dans simulation  0\n",
      "Nombre de 0 : 74\n",
      "Nombre de 1 : 9\n",
      "Nombre de -1 : 417\n",
      "dans simulation  1\n",
      "Nombre de 0 : 132\n",
      "Nombre de 1 : 1\n",
      "Nombre de -1 : 367\n",
      "dans simulation  2\n",
      "Nombre de 0 : 160\n",
      "Nombre de 1 : 0\n",
      "Nombre de -1 : 340\n",
      "dans simulation  3\n",
      "Nombre de 0 : 161\n",
      "Nombre de 1 : 0\n",
      "Nombre de -1 : 339\n"
     ]
    }
   ],
   "source": [
    "#je vais ici simuler 100 parties afin de voir a quelle frequence il y a \n",
    "#victoire pour tenter de determiner lexistence de strategies gagnantes\n",
    "\n",
    "game = NDGame(3,2)\n",
    "\n",
    "\n",
    "synthese = []\n",
    "\n",
    "for k in range(4):\n",
    "\n",
    "\n",
    "    player = 1\n",
    "\n",
    "    model = ResNet(game, 4, 64, device)\n",
    "    model.load_state_dict(torch.load( f'model_{k}_200_4_1000_10_3².pt', map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "    args = {\n",
    "        'C': 2,\n",
    "        'num_searches': 500,\n",
    "        'dirichlet_epsilon': 0.,\n",
    "        'dirichlet_alpha': 0.3\n",
    "    }\n",
    "    model.eval()\n",
    "\n",
    "    mcts = MCTS(game, args, model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def joue_contre_soi(n_parties):\n",
    "        for i in range(n_parties):\n",
    "            state = game.get_initial_state()\n",
    "            player = 1\n",
    "            while True:\n",
    "\n",
    "                neutral_state = game.change_perspective(state, player)\n",
    "                mcts_probs = mcts.search(neutral_state)\n",
    "                action = np.argmax(mcts_probs)\n",
    "                #print(state)\n",
    "                #plt.bar(range(game.action_size), mcts_probs)\n",
    "                #plt.show()\n",
    "\n",
    "                state = game.get_next_state(state, action, player)\n",
    "\n",
    "                value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "                if is_terminal:\n",
    "                    if value == 1:\n",
    "                        print(player, \"won\")\n",
    "                        resultats.append(player)\n",
    "                        print(state)\n",
    "                    else:\n",
    "                        print(\"draw\")\n",
    "                        print(state)\n",
    "                        resultats.append(0)\n",
    "                    break\n",
    "\n",
    "                player = game.get_opponent(player)\n",
    "\n",
    "\n",
    "    def joue_contre_rand(n_parties):\n",
    "        for i in range(n_parties):\n",
    "            state = game.get_initial_state()\n",
    "            player = 1\n",
    "            while True:\n",
    "                if player == 1:\n",
    "                    valid_moves = game.get_valid_moves(state)\n",
    "                    tab = nd.get_valid_moves(state)\n",
    "                    indices = np.where(tab == 1)[0]        # prend les indices valant 1\n",
    "                    action = np.random.choice(indices)\n",
    "\n",
    "                else:\n",
    "                    neutral_state = game.change_perspective(state, player)\n",
    "                    mcts_probs = mcts.search(neutral_state)\n",
    "                    action = np.argmax(mcts_probs)\n",
    "\n",
    "                state = game.get_next_state(state, action, player)\n",
    "\n",
    "                value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "                if is_terminal:\n",
    "                    if value == 1:\n",
    "                        resultats.append(player)\n",
    "                    else:\n",
    "                        resultats.append(0)\n",
    "                    break\n",
    "\n",
    "                player = game.get_opponent(player)\n",
    "\n",
    "    resultats = []\n",
    "    joue_contre_rand(1000)\n",
    "    nb_zero = resultats.count(0)\n",
    "    nb_un = resultats.count(1)\n",
    "    nb_moins_un = resultats.count(-1)\n",
    "    print(\"dans simulation \", k)\n",
    "    print(\"Nombre de 0 :\", nb_zero)\n",
    "    print(\"Nombre de 1 :\", nb_un)\n",
    "    print(\"Nombre de -1 :\", nb_moins_un)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = NDGame(3,2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state = game.get_initial_state()\n",
    "print(state)\n",
    "state = game.get_next_state(state, 2, 1)\n",
    "state = game.get_next_state(state, 4, 1)\n",
    "state = game.get_next_state(state, 6, -1)\n",
    "state = game.get_next_state(state, 2, -1)\n",
    "\n",
    "\n",
    "encoded_state = game.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(game, 4, 64, device = device)\n",
    "model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(game.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
